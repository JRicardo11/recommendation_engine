{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practice_tagging.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO4fGjy7Gvni1YJC40mZoMX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JRicardo11/recommendation_engine/blob/main/Practice_tagging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8cxb7p2wDGD",
        "outputId": "adeb246c-e477-4f5b-b664-298d8d5894cb"
      },
      "source": [
        "! git clone https://github.com/acastellanos-ie/MBD-EN-BL-ENE-2020-J-1.git"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'MBD-EN-BL-ENE-2020-J-1'...\n",
            "remote: Enumerating objects: 4485, done.\u001b[K\n",
            "remote: Counting objects: 100% (4485/4485), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4372/4372), done.\u001b[K\n",
            "remote: Total 4485 (delta 161), reused 4387 (delta 94), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (4485/4485), 13.41 MiB | 18.31 MiB/s, done.\n",
            "Resolving deltas: 100% (161/161), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdQm5VawyRw0",
        "outputId": "fe49ef69-3ecc-465e-e6a7-4e7871ee6e27"
      },
      "source": [
        "! pip install -Uqqr MBD-EN-BL-ENE-2020-J-1/requirements.txt"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.5MB 6.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.4MB 28.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 12.0MB 168kB/s \n",
            "\u001b[K     |████████████████████████████████| 9.9MB 42.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 348kB 30.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 57.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 727kB 39.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 454.3MB 37kB/s \n",
            "\u001b[K     |████████████████████████████████| 25.3MB 1.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 8.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.3MB 19.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8MB 26.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 36.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 8.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 38.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 7.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 34.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.0MB 35.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.0MB 18.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 471kB 54.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.0MB 35.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 983kB 36.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 266kB 37.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 471kB 52.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 901kB 57.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3MB 35.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 6.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.9MB 24.0MB/s \n",
            "\u001b[?25h  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ktrain (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for eli5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for stellargraph (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for networkx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for syntok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement gast==0.3.3, but you'll have gast 0.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement grpcio~=1.32.0, but you'll have grpcio 1.34.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement h5py~=2.10.0, but you'll have h5py 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement tensorflow-estimator<2.5.0,>=2.4.0, but you'll have tensorflow-estimator 2.5.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.1.0; python_version >= \"3.0\", but you'll have pandas 1.2.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: ktrain 0.26.2 has requirement scikit-learn==0.23.2, but you'll have scikit-learn 0.22.2.post1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: ktrain 0.26.2 has requirement transformers<=4.3.3,>=4.0.0, but you'll have transformers 4.6.0 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnAZxmozwKTL",
        "outputId": "05c38575-e68f-4c5f-87c0-2617a0a704ae"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "text = nltk.word_tokenize(\"They refuse to permit us to obtain the refuse permit\")\n",
        "nltk.pos_tag(text)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('They', 'PRP'),\n",
              " ('refuse', 'VBP'),\n",
              " ('to', 'TO'),\n",
              " ('permit', 'VB'),\n",
              " ('us', 'PRP'),\n",
              " ('to', 'TO'),\n",
              " ('obtain', 'VB'),\n",
              " ('the', 'DT'),\n",
              " ('refuse', 'NN'),\n",
              " ('permit', 'NN')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFsworK3wlBu",
        "outputId": "7558479f-0330-415d-e314-768721493ef1"
      },
      "source": [
        "nltk.download('brown')\n",
        "text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())\n",
        "text.similar('woman')\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "man time day year car moment world house family child country boy\n",
            "state job place way war girl work word\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDR9d1hzzPt_",
        "outputId": "f4f8b342-0b93-4764-f6a6-f3978584513d"
      },
      "source": [
        "tagged_token = nltk.tag.str2tuple('fly/NN')\n",
        "tagged_token"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fly', 'NN')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gxhETNAzUis",
        "outputId": "bf7bbba3-85f5-4509-df90-831f1c7807c1"
      },
      "source": [
        "nltk.corpus.brown.tagged_words()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'AT'), ('Fulton', 'NP-TL'), ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "woqgq7Kuwzc0",
        "outputId": "3167cb7f-5469-42ad-800d-83aa855718ec"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "nltk.download('state_union')\n",
        "from nltk.corpus import state_union\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
        "sample_text= state_union.raw(\"2006-GWBush.txt\")\n",
        "\n",
        "custom_sent_tokenizer = word_tokonize(train_text)\n",
        "\n",
        "tokenized = custom_sent_tokenizer(sample_text)\n",
        "\n",
        "def process_content():\n",
        "    try:\n",
        "      for i in tokenized:\n",
        "        words = nltk.word_tokenize(i)\n",
        "        tagged = nltk.pos_tag(words)\n",
        "    \n",
        "    except Exception as e:\n",
        "      print(str(e))\n",
        "\n",
        "\n",
        "process_content()\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]   Package state_union is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-51e0a9e59b2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0msample_text\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mstate_union\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2006-GWBush.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mcustom_sent_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokonize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_sent_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'word_tokonize' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szadwTuJyORw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}