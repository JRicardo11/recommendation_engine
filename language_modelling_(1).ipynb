{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "language_modelling (1).ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JRicardo11/recommendation_engine/blob/main/language_modelling_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCvwbQnTvBRh"
      },
      "source": [
        "# Google Colab Configuration\n",
        "\n",
        "**Execute this steps to configure the Google Colab environment in order to execute this notebook. It is not required if you are executing it locally and you have properly configured your local environment according to what explained in the Github Repository.**\n",
        "\n",
        "The first step is to clone the repository to have access to all the data and files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d7mC64KvlwP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4a0fa82-21ee-492a-c1ed-83d2e4d43ff7"
      },
      "source": [
        "! git clone https://github.com/acastellanos-ie/MBD-EN-BL-ENE-2020-J-1.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'MBD-EN-BL-ENE-2020-J-1'...\n",
            "remote: Enumerating objects: 4499, done.\u001b[K\n",
            "remote: Counting objects: 100% (4499/4499), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4319/4319), done.\u001b[K\n",
            "remote: Total 4499 (delta 169), reused 4482 (delta 161), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (4499/4499), 13.45 MiB | 20.71 MiB/s, done.\n",
            "Resolving deltas: 100% (169/169), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ecfec2Y4v6e9"
      },
      "source": [
        "Install the requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIep7l0jvtUB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00a74074-50cd-447b-e66e-3eb075845297"
      },
      "source": [
        "! pip install -Uqqr MBD-EN-BL-ENE-2020-J-1/requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.5MB 6.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.4MB 10.5MB/s \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK0Z9HRhyvza"
      },
      "source": [
        "Ensure that you have the GPU runtime activated:\n",
        "\n",
        "![](https://miro.medium.com/max/3006/1*vOkqNhJNl1204kOhqq59zA.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHDzMQwpyODo"
      },
      "source": [
        "Now you have everything you need to execute the code in Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-qIO7sguzTC"
      },
      "source": [
        "# Language Modelling\n",
        "\n",
        "In this notebook we are going to start playing with languages models. In particular, we are going to start with the simplest approach based on n-grams. Then, in the following threads, we will move to more advanced approaches based on LSTM and Transformer architectures.\n",
        "\n",
        "The Natural Language Toolkit (NLTK) has data types and functions that make life easier for us when we want to count bigrams and compute their probabilities.\n",
        "\n",
        "Let's start!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIT1xuFUuzTK"
      },
      "source": [
        "import nltk\n",
        "nltk.download('brown')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcOKzXu1uzTL"
      },
      "source": [
        "**Import the Brown corpus**\n",
        "\n",
        "For the experimentation, we are going to use the well-known Brown Corpus.\n",
        "\n",
        "The Brown University Standard Corpus of Present-Day American Englis, or just Brown Corpus (https://en.wikipedia.org/wiki/Brown_Corpus),  is a general corpus containing 500 samples of English-language text, totaling roughly one million words, compiled from works published in the United States in 1961.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2SYcsnQuzTL"
      },
      "source": [
        "from nltk.corpus import brown\n",
        "brown.categories()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9g71QARuzTM"
      },
      "source": [
        "From the words of the Brown corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoR82bqxuzTN"
      },
      "source": [
        "brown.words()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNIIvekauzTN"
      },
      "source": [
        "Let's inspect what are the most likely (most frequent) words in the dataset. The probability of a word is very important for our language model. When we ask the LM to generate new text, it should rely on these word probabilities, so it can generate words that are likely in our dataset.\n",
        "\n",
        "We compute the word frequency by using the `FreqDist` function of NLTK (an nltk.FreqDist() is like a dictionary, but it is ordered by frequency)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tq7nSgs0uzTN"
      },
      "source": [
        "The following uses this function to compute the freqs and plot the 20 most frequent words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ousstIaUuzTO"
      },
      "source": [
        "freq_brown = nltk.FreqDist(brown.words())\n",
        "\n",
        "list(freq_brown.keys())[:20]\n",
        "freq_brown.most_common(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6N5aGMWyuzTO"
      },
      "source": [
        "We can see that they are mostly stopwords, punctuation signs.\n",
        "\n",
        "**Should we remove them? Why?** \n",
        "\n",
        "No, just think in what we are trying to do here. We are trying to use the dataset to create a model of the language to, given a set of words, predict the most probable next word. For this process, stopwords, as well as punctuation or other signs are need.\n",
        "\n",
        "For the same reason, we shall not stemmize/lemmatize, neither normalize the words. We need all these variations to learn a proper language model (i.e, `the` != `The`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHWVvOy8uzTP"
      },
      "source": [
        "## Bigram Model\n",
        "\n",
        "We'll start small and we will create a language model based on bi-grams. This LM is rather simplistic: it will only codify relationships of length 2.\n",
        "\n",
        "To that end, we will use the `ConditionalFreqDist` function of NLTK. `nltk.ConditionalFreqDist()` counts frequencies of pairs. When given a list of bigrams, it maps each first word of a bigram to a FreqDist over the second words of the bigram.\n",
        "\n",
        "If you remember the theoretical session, we are applying the Markov assumption: the next element (word in our case) of a sequence can be predicted by just focusing on the previous one.\n",
        "\n",
        "The following code creates these bi-gram counts.\n",
        "If we print the `conditions` we can see the antecedent of the bi-grams. (`conditions()` in a `ConditionalFreqDist` are like `keys()` in a dictionary)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLqSKRGluzTQ"
      },
      "source": [
        "cfreq_brown_2gram = nltk.ConditionalFreqDist(nltk.bigrams(brown.words()))\n",
        "cfreq_brown_2gram.conditions()[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FyBBzaQuzTR"
      },
      "source": [
        "Let' see the most frequent terms after the word `my`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HZlMa8GuzTR"
      },
      "source": [
        "# the cfreq_brown_2gram entry for \"my\" is a FreqDist (i.e, a dictionary of word and freqCount).\n",
        "my_terms = cfreq_brown_2gram[\"my\"]\n",
        "\n",
        "# Sort the terms by frequency and print the 25th most common\n",
        "sorted(my_terms.items(), key=lambda x: -x[1])[:25]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWvLg4L1uzTR"
      },
      "source": [
        "We can do the same with the `most_common` function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eVkxT7DuzTS"
      },
      "source": [
        "cfreq_brown_2gram[\"my\"].most_common(25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gtx3xuTuzTS"
      },
      "source": [
        "With the `nltk.ConditionalProbDist()`, map pairs are mapped to probabilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9Vk1D32uzTS"
      },
      "source": [
        "cprob_brown_2gram = nltk.ConditionalProbDist(cfreq_brown_2gram, nltk.MLEProbDist) # Uses a Maximum Likelihood Estimation (MLE) estimator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f8piubquzTS"
      },
      "source": [
        "This again has `conditions()` wihch are like dictionary keys"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeqSahgxuzTT"
      },
      "source": [
        "cprob_brown_2gram.conditions()[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEIp1wo_uzTT"
      },
      "source": [
        "We can also find the words that can come after `my` by using the function `samples()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOBM_KLZuzTT"
      },
      "source": [
        "cprob_brown_2gram[\"my\"].samples()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8EPw4y0uzTT"
      },
      "source": [
        "In addition, you can see the prob of a particular pair"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fF3rUahluzTU"
      },
      "source": [
        "cprob_brown_2gram[\"my\"].prob(\"own\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPjydGm4uzTU"
      },
      "source": [
        "cprob_brown_2gram[\"my\"].prob(\"leg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpzMKz1UuzTU"
      },
      "source": [
        "## Compute the probability of a sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXtPHiQ7uzTV"
      },
      "source": [
        "Create a function to compute the probability of a word from its frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Josl8K1uzTV"
      },
      "source": [
        "def unigram_prob(word):\n",
        "    len_brown = len(brown.words())\n",
        "    return float(freq_brown[word]) / float(len_brown)\n",
        "\n",
        "unigram_prob(\"night\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc9Fa4-1uzTV"
      },
      "source": [
        "We now can ask for the probability of a word sequence.\n",
        "\n",
        "For instance: `P(how do you do) = P(how) * P(do|how) * P(you|do) * P(do | you)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12JoyG54uzTV"
      },
      "source": [
        "unigram_prob(\"how\") * cprob_brown_2gram[\"how\"].prob(\"do\") * cprob_brown_2gram[\"do\"].prob(\"you\") * cprob_brown_2gram[\"you\"].prob(\"do\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkCsamOiuzTW"
      },
      "source": [
        "Compare it with the prob of another not so common sentence: `how do you dance`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CI49ifcUuzTW"
      },
      "source": [
        "unigram_prob(\"how\") * cprob_brown_2gram[\"how\"].prob(\"do\") * cprob_brown_2gram[\"do\"].prob(\"you\") * cprob_brown_2gram[\"you\"].prob(\"dance\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSb3PvTruzTX"
      },
      "source": [
        "As expected, one order of magnitude less probable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWT8NGKkuzTX"
      },
      "source": [
        "## Generate Language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vJk7pcpuzTX"
      },
      "source": [
        "With our bi-gram language model already generated, we can now use it to generate text and see what has our model learned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjCHMKxGuzTY"
      },
      "source": [
        "cprob_brown_2gram[\"my\"].generate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqZngBJyuzTY"
      },
      "source": [
        "Let's see if the model create valid text or just jiberish"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVS1qI2cuzTY"
      },
      "source": [
        "word = \"my\"\n",
        "text = \"\"\n",
        "for index in range(20):\n",
        "    text += word + \" \"\n",
        "    word = cprob_brown_2gram[ word].generate()\n",
        "print(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Wdev8EPuzTZ"
      },
      "source": [
        "It is not a valid sentence, but it has some kind of sense. \n",
        "\n",
        "Remember that we are just learning from bigrams!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaZbER2euzTZ"
      },
      "source": [
        "**We can try another datasets to train a language models using different dataset.**\n",
        "\n",
        "In particular we are going to import the book dataset of NLTK, which includes the text of different books.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkaYqUZ5uzTZ"
      },
      "source": [
        "The following function takes a text (i.e., the text o a given book) to learn a language model, and a initial word to start the generation and the number of words that have to be generated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3hpAVo9uzTZ"
      },
      "source": [
        "# Here is how to do this with NLTK books:\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('genesis')\n",
        "nltk.download('inaugural')\n",
        "nltk.download('nps_chat')\n",
        "nltk.download('webtext')\n",
        "nltk.download('treebank')\n",
        "\n",
        "from nltk.book import *\n",
        "\n",
        "\n",
        "def generate_text(text, initialword, numwords):\n",
        "    bigrams = list(nltk.ngrams(text, 2))\n",
        "    cpd = nltk.ConditionalProbDist(nltk.ConditionalFreqDist(bigrams), nltk.MLEProbDist)\n",
        "\n",
        "    word = initialword\n",
        "    text = \"\"\n",
        "    for i in range(numwords):\n",
        "        text += word + \" \"\n",
        "        word = cpd[ word].generate() \n",
        "\n",
        "    print(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVhYqc5HuzTa"
      },
      "source": [
        "We use different books to generate text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVD06Gf8uzTa"
      },
      "source": [
        "# Holy Grail\n",
        "generate_text(text6, \"I\", 25)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXk0K-vzuzTa"
      },
      "source": [
        "# sense and sensibility\n",
        "generate_text(text2, \"I\", 25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4s6o2hHuzTa"
      },
      "source": [
        "# TriGrams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssCJc1-iuzTb"
      },
      "source": [
        "Let's try a more advance model using tri-grams to see if it is able to generate better language.\n",
        "\n",
        "We cannot use the `ConditionalFreqDist` as before. `nltk.ConditionalFreqDist` expects its data as a sequence of `(condition, item)` tuples. `nltk.trigrams` returns tuples of length 3. Therefore, we have to adapt the trigrams output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRZ9Rm1NuzTb"
      },
      "source": [
        "def generate_text(text, initialword, numwords):\n",
        "    trigrams = list(nltk.ngrams(text, 3,  pad_right=True, pad_left=True))\n",
        "    trigram_pairs = (((w0, w1), w2) for w0, w1, w2 in trigrams) # Adapt the format to use ConditionalFreqDist\n",
        "    cpd = nltk.ConditionalProbDist(nltk.ConditionalFreqDist(trigram_pairs), nltk.MLEProbDist)\n",
        "\n",
        "    word = initialword\n",
        "    text = \"\"\n",
        "    for i in range(numwords):\n",
        "        w = cpd[(word[i],word[i+1])].generate() \n",
        "        word += [w]\n",
        "    \n",
        "    print(\" \".join(word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxWLWg55uzTb"
      },
      "source": [
        "generate_text(text2, [\"I\", \"am\"], 25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlNYg8JMAQqk"
      },
      "source": [
        "generate_text(text2, [\"I\", \"will\"], 25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsDDHFXPuzTc"
      },
      "source": [
        "As expected, it creates a better lm.\n",
        "\n",
        "Can we go on with more n-grams? Let's see"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0Y1TnU7uzTc"
      },
      "source": [
        "# N-grams\n",
        "\n",
        "We are going to update again the `generate_text` function to create a language model based on 4-grams.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lI4GDN_JuzTc"
      },
      "source": [
        "def generate_text(text, initialword, numwords):\n",
        "    ngrams = list(nltk.ngrams(text, 4,  pad_right=True, pad_left=True))\n",
        "    ngram_pairs = (((w0, w1, w2), w3) for w0, w1, w2, w3 in ngrams)\n",
        "    cpd = nltk.ConditionalProbDist(nltk.ConditionalFreqDist(ngram_pairs), nltk.MLEProbDist)\n",
        "\n",
        "    word = initialword\n",
        "    text = \"\"\n",
        "    for i in range(numwords):\n",
        "        w = cpd[(word[i],word[i+1], word[i+2])].generate() \n",
        "        word += [w]\n",
        "    \n",
        "    print(\" \".join(word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_cKydeOAskk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIcg_O0_uzTc"
      },
      "source": [
        "generate_text(text2, [\"I\", \"am\", \"very\"], 25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBjSpnNwuzTd"
      },
      "source": [
        "As we make the n-grams larger we got more accurate language models. However, as explained in class, if we create large n-grams we are not going to have enough data to train our models: we will never see enough data (enough sequences of n-grams) to train the model.\n",
        "\n",
        "As an exercise, I leave up to you to keep extending this LM model to 5-gram, 6-gram....\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07SpBB58AtiW"
      },
      "source": [
        "def generate_text(text, initialword, numwords):\n",
        "    ngrams = list(nltk.ngrams(text, 6,  pad_right=True, pad_left=True))\n",
        "    ngram_pairs = (((w0, w1, w2,w3,w4), w5) for w0, w1, w2, w3, w4, w5 in ngrams)\n",
        "    cpd = nltk.ConditionalProbDist(nltk.ConditionalFreqDist(ngram_pairs), nltk.MLEProbDist)\n",
        "\n",
        "    word = initialword\n",
        "    text = \"\"\n",
        "    for i in range(numwords):\n",
        "        w = cpd[(word[i],word[i+1], word[i+2])].generate() \n",
        "        word += [w]\n",
        "    \n",
        "    print(\" \".join(word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvS2u0q4A7vq"
      },
      "source": [
        "generate_text(text, [\"I\", \"am\"], 25)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}